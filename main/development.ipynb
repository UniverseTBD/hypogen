{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/cdt_cw_5265_z_gwnxlf0tv80000gn/T/ipykernel_48419/3535194879.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('arxiv.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(327288, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('arxiv.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"  We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy\\nobservations of the Serpens star-forming region. In particular we present a set\\nof criteria for isolating bona fide young stellar objects, YSO's, from the\\nextensive background contamination by extra-galactic objects. We then discuss\\nthe properties of the resulting high confidence set of YSO's. We find 235 such\\nobjects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An\\nadditional set of 51 lower confidence YSO's outside this area is identified\\nfrom the MIPS data combined with 2MASS photometry. We describe two sets of\\nresults, color-color diagrams to compare our observed source properties with\\nthose of theoretical models for star/disk/envelope systems and our own modeling\\nof the subset of our objects that appear to be star+disks. These objects\\nexhibit a very wide range of disk properties, from many that can be fit with\\nactively accreting disks to some with both passive disks and even possibly\\ndebris disks. We find that the luminosity function of YSO's in Serpens extends\\ndown to at least a few x .001 Lsun or lower for an assumed distance of 260 pc.\\nThe lower limit may be set by our inability to distinguish YSO's from\\nextra-galactic sources more than by the lack of YSO's at very low luminosities.\\nA spatial clustering analysis shows that the nominally less-evolved YSO's are\\nmore highly clustered than the later stages and that the background\\nextra-galactic population can be fit by the same two-point correlation function\\nas seen in other extra-galactic studies. We also present a table of matches\\nbetween several previous infrared and X-ray studies of the Serpens YSO\\npopulation and our Spitzer data set.\\n\",\n",
       "       '  Results from spectroscopic observations of the Intermediate Polar (IP) EX Hya\\nin quiescence during 1991 and 2001 are presented. Spin-modulated radial\\nvelocities consistent with an outer disc origin were detected for the first\\ntime in an IP. The spin pulsation was modulated with velocities near ~500-600\\nkm/s. These velocities are consistent with those of material circulating at the\\nouter edge of the accretion disc, suggesting corotation of the accretion\\ncurtain with material near the Roche lobe radius. Furthermore, spin Doppler\\ntomograms have revealed evidence of the accretion curtain emission extending\\nfrom velocities of ~500 km/s to ~1000 km/s. These findings have confirmed the\\ntheoretical model predictions of King & Wynn (1999), Belle et al. (2002) and\\nNorton et al. (2004) for EX Hya, which predict large accretion curtains that\\nextend to a distance close to the Roche lobe radius in this system. Evidence\\nfor overflow stream of material falling onto the magnetosphere was observed,\\nconfirming the result of Belle et al. (2005) that disc overflow in EX Hya is\\npresent during quiescence as well as outburst. It appears that the hbeta and\\nhgamma spin radial velocities originated from the rotation of the funnel at the\\nouter disc edge, while those of halpha were produced due to the flow of\\nmaterial along the field lines far from the white dwarf (narrow component) and\\nclose to the white dwarf (broad-base component), in agreement with the\\naccretion curtain model.\\n',\n",
       "       '  The very nature of the solar chromosphere, its structuring and dynamics,\\nremains far from being properly understood, in spite of intensive research.\\nHere we point out the potential of chromospheric observations at millimeter\\nwavelengths to resolve this long-standing problem. Computations carried out\\nwith a sophisticated dynamic model of the solar chromosphere due to Carlsson\\nand Stein demonstrate that millimeter emission is extremely sensitive to\\ndynamic processes in the chromosphere and the appropriate wavelengths to look\\nfor dynamic signatures are in the range 0.8-5.0 mm. The model also suggests\\nthat high resolution observations at mm wavelengths, as will be provided by\\nALMA, will have the unique property of reacting to both the hot and the cool\\ngas, and thus will have the potential of distinguishing between rival models of\\nthe solar atmosphere. Thus, initial results obtained from the observations of\\nthe quiet Sun at 3.5 mm with the BIMA array (resolution of 12 arcsec) reveal\\nsignificant oscillations with amplitudes of 50-150 K and frequencies of 1.5-8\\nmHz with a tendency toward short-period oscillations in internetwork and longer\\nperiods in network regions. However higher spatial resolution, such as that\\nprovided by ALMA, is required for a clean separation between the features\\nwithin the solar atmosphere and for an adequate comparison with the output of\\nthe comprehensive dynamic simulations.\\n',\n",
       "       ...,\n",
       "       '  Fidelity plays a key role in quantum information and communication theory.\\nFidelity can be interpreted as the probability that a decoded message possesses\\nthe same information content as the message prior to coding and transmission.\\nIn this paper, we give a formula of Bures fidelity for displaced squeezed\\nthermal states directly by the displacement and squeezing parameters and\\nbirefly discuss how the results can apply to quantum information theory.\\n',\n",
       "       '  Newtonian dynamical systems accepting the normal shift on an arbitrary\\nRiemannian manifold are considered. Partial differential equations forming the\\nweak and additional normality conditions for them are reported.\\n',\n",
       "       '  The vector Burgers equation is extended to include pressure gradients and\\ngravity. It is shown that within the framework of the Cole-Hopf transformation\\nthere are no physical solutions to this problem. This result is important\\nbecause it clearly demonstrates that any extension of Burgers equation to more\\ninteresting physical situations is strongly limited.\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hypotheses extracted: 89\n"
     ]
    }
   ],
   "source": [
    "# Read in extracted_large.json and print how many hypotheses were extracted:\n",
    "import json\n",
    "\n",
    "with open('extracted_large.json', 'r') as f:\n",
    "    hypotheses = json.load(f)\n",
    "\n",
    "print(f\"Number of hypotheses extracted: {len(hypotheses)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Critic Chain: Takes a hypothesis and provides a critique.\n",
    "critic_template = \"\"\"\n",
    "Given the following hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "Analyze each part of the hypothesis:\n",
    "1. Clarity: Are the statements clear and easy to understand?\n",
    "2. Coherence: Do the different parts of the hypothesis logically flow together? Does it seem like the proposed solution would work?\n",
    "3. Scientific Validity: Are there any scientific inaccuracies or assumptions that seem unfounded?\n",
    "\n",
    "After your analysis, provide specific feedback on how each field (Problem, Solution, Methodology, Evaluation, Results) can be improved.\n",
    "\"\"\"\n",
    "critic_prompt = PromptTemplate(input_variables=[\"hypothesis\"], template=critic_template)\n",
    "critic_chain = LLMChain(llm=llm, prompt=critic_prompt, output_key=\"critique\")\n",
    "\n",
    "# Reviser Chain: Takes original hypothesis and critique, then provides a revised hypothesis.\n",
    "reviser_template = \"\"\"\n",
    "Given the original hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "And based on the critique provided:\n",
    "{critique}\n",
    "\n",
    "Revise the hypothesis by addressing each point of the critique. Ensure the new version is clearer, more coherent, and scientifically valid.\n",
    "\"\"\"\n",
    "reviser_prompt = PromptTemplate(input_variables=[\"hypothesis\", \"critique\"], template=reviser_template)\n",
    "reviser_chain = LLMChain(llm=llm, prompt=reviser_prompt, output_key=\"revised_hypothesis\")\n",
    "\n",
    "# Overall Sequential Chain: Critiques the hypothesis and then revises it.\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[critic_chain, reviser_chain],\n",
    "    input_variables=[\"hypothesis\"],\n",
    "    output_variables=[\"critique\", \"revised_hypothesis\"],\n",
    "    verbose=True)\n",
    "\n",
    "# Test the chain with a provided hypothesis\n",
    "hypothesis = {\n",
    "    \"Problem\": \"Issues in determining basic characteristics of black holes and their surrounding disks in X-ray binary, using models of the source's disk X-ray continuum. A key issue is the determination of the \\\"color correction factor\\\".\",\n",
    "    \"Solution\": \"Using observational data to estimate the color correction factor by modeling the disk spectrum with saturated Compton scattering.\",\n",
    "    \"Methodology\": \"The work is based on two observations made by XMM-Newton on GX 339-4. These observations offer high-quality data at low energies. The spectra were then fitted to these models.\",\n",
    "    \"Evaluation\": \"The quality of fit of the spectra to the models was examined. Other models were also tested for fit.\",\n",
    "    \"Results\": \"The spectra fits well with the model and provides reasonable values for the color correction factor. However, the high-soft-state continuum cannot be adequately fitted by the latest disk models.\"\n",
    "}\n",
    "result = overall_chain({\"hypothesis\": str(hypothesis)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis:\n",
      "\n",
      "1. Clarity: The hypothesis is relatively clear but contains technical terms that may be difficult for non-experts to understand. For example, terms like \"X-ray binary\", \"color correction factor\", \"saturated Compton scattering\", and \"high-soft-state continuum\" may require more explanation or context.\n",
      "   \n",
      "2. Coherence: The different parts of the hypothesis logically flow together. The problem is clearly stated, a solution is proposed, the methodology is explained, an evaluation process is detailed, and results are provided. \n",
      "\n",
      "3. Scientific Validity: The hypothesis appears to be scientifically valid. It uses real-world observational data and incorporates established scientific models. However, the validity of the results would depend on the accuracy and reliability of the data and models used.\n",
      "\n",
      "Feedback:\n",
      "\n",
      "Problem: The problem statement could be improved by providing more background information to help non-experts understand the issue. For example, briefly explaining what a black hole is, what an X-ray binary is, and why determining their characteristics is important.\n",
      "\n",
      "Solution: The solution could be enhanced by explaining why modeling the disk spectrum with saturated Compton scattering is an effective approach to estimating the color correction factor.\n",
      "\n",
      "Methodology: The methodology could be improved by providing more details about the process. For example, how were the spectra fitted to the models? What specific models were used? \n",
      "\n",
      "Evaluation: The evaluation could be enhanced by specifying the metrics or criteria used to assess the quality of fit. Also, it would be beneficial to know which other models were tested.\n",
      "\n",
      "Results: The results could be improved by providing more specific information. For instance, what are the \"reasonable values\" for the color correction factor? In what ways were the latest disk models unable to adequately fit the high-soft-state continuum?\n"
     ]
    }
   ],
   "source": [
    "print(result['critique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Problem': 'The study aims to determine basic attributes of black holes and their surrounding disks, specifically in X-ray binary systems. In simple terms, X-ray binaries are systems where a black hole and a star are in close proximity, and the intense gravitational pull of the black hole causes matter from the star to form a disk around the black hole. The main challenge is the determination of the \"color correction factor\", a numerical factor that helps us to understand the temperature and energy of the emitting source.', \n",
      "\n",
      "'Solution': 'The proposed solution is to estimate the color correction factor using observational data. This is achieved by modeling the spectrum of the disk around the black hole with a phenomenon known as saturated Compton scattering, where photons gain energy by interacting with high-energy particles.', \n",
      "\n",
      "'Methodology': 'The research utilized two observational datasets provided by the XMM-Newton space telescope on the X-ray binary system GX 339-4. The high-quality, low-energy data from these observations was analyzed and the spectrum of the disk was fitted to models based on the principles of saturated Compton scattering.', \n",
      "\n",
      "'Evaluation': 'The evaluation of this method involved checking the quality of the fit of the observed spectra to the proposed models. The quality of fit was assessed based on the residual sum of squares, a common metric used in spectral analysis. Furthermore, the fit was also compared with other existing models for black hole disks.', \n",
      "\n",
      "'Results': 'The observational spectra fit well with the proposed model, providing color correction factor values within the expected range for X-ray binaries. However, when applied to the high-soft-state continuum, a condition where the X-ray output is dominated by thermal emissions from the disk, the most recent models for black hole disks were found to be inadequate.'}\n"
     ]
    }
   ],
   "source": [
    "print(result['revised_hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "hypothesis_dict = ast.literal_eval(result['revised_hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import yaml\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import SimpleMemory\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This is an LLMChain to write a synopsis given a title of a play.\n",
    "config = yaml.safe_load(open(\"../config.yml\"))\n",
    "API_KEY = config['api_key']\n",
    "DEPLOYMENT_NAME = config['deployment_name']\n",
    "BASE_URL = config['base_url']\n",
    "API_VERSION = config['api_version']\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=BASE_URL,\n",
    "    openai_api_version=API_VERSION,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "# Critic Chain: Takes a hypothesis and provides a critique.\n",
    "critic_template = \"\"\"\n",
    "Given the following hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "Analyze each part of the hypothesis:\n",
    "1. Clarity: Are the statements clear and easy to understand?\n",
    "2. Coherence: Do the different parts of the hypothesis logically flow together? Does it seem like the proposed solution would work?\n",
    "3. Scientific Validity: Are there any scientific inaccuracies or assumptions that seem unfounded?\n",
    "\n",
    "After your analysis, provide specific feedback on how each field (Problem, Solution, Methodology, Evaluation, Results) can be improved.\n",
    "Specifically, give feedback on the frailties of the idea as a whole, and suggest potential enhancements.\n",
    "\"\"\"\n",
    "\n",
    "# Reviser Chain: Takes original hypothesis and critique, then provides a revised hypothesis.\n",
    "reviser_template = \"\"\"\n",
    "Given the original hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "And based on the critique provided:\n",
    "{critique}\n",
    "\n",
    "Revise the hypothesis by addressing each point of the critique. Ensure the new version is clearer, more coherent, and scientifically valid.\n",
    "\"\"\"\n",
    "\n",
    "def adversarial_update_hypothesis(hypothesis):\n",
    "    critic_prompt = PromptTemplate(input_variables=[\"hypothesis\"], template=critic_template)\n",
    "    critic_chain = LLMChain(llm=llm, prompt=critic_prompt, output_key=\"critique\")\n",
    "    reviser_prompt = PromptTemplate(input_variables=[\"hypothesis\", \"critique\"], template=reviser_template)\n",
    "    reviser_chain = LLMChain(llm=llm, prompt=reviser_prompt, output_key=\"revised_hypothesis\")\n",
    "    overall_chain = SequentialChain(\n",
    "        chains=[critic_chain, reviser_chain],\n",
    "        input_variables=[\"hypothesis\"],\n",
    "        output_variables=[\"critique\", \"revised_hypothesis\"],\n",
    "        verbose=False)\n",
    "    result = overall_chain({\"hypothesis\": str(hypothesis)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00c076dca1744538db6e9378afc34c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'{\\'Problem\\': \\'We are tasked with the challenge of accurately determining the fundamental features of black holes and the disks that surround them in X-ray binary, a type of binary star system that emits X-rays. This process is pivotal to enhancing our understanding of how matter behaves in extreme gravitational fields and the nature of black holes. The importance of achieving this lies in its potential to contribute significantly to the broader field of astrophysics, enabling us to understand the universe better. The critical part of this process is the determination of the \"color correction factor\", a measure of how the color of an object appears to change due to relativistic effects when it is moving at speeds close to the speed of light.\\',   \\'Solution\\': \\'We propose a solution that utilizes observational data to derive estimates for the color correction factor. This involves modeling the disk spectrum through the innovative concept of \"saturated Compton scattering\". In this process, photons gain energy by interacting with charged particles, allowing us to account for complexities in the radiation process. Saturated Compton scattering is particularly useful since it provides a more precise estimation of the color correction factor by accounting for the shifts in energy and wavelength of emitted photons. The advantage of this method lies in its ability to provide a more refined and accurate estimate of the color correction factor, which is central to our understanding of black holes.\\',   \\'Methodology\\': \\'Our methodology revolved around using two specific observations made by the XMM-Newton on the X-ray source, GX 339-4. The choice of GX 339-4 was based on its ability to provide high-quality data at low energies, essential for an accurate estimation of the color correction factor. Low-energy observations offer clearer spectral lines, which allows for more accurate modeling and estimation. The selection of these specific observations was strategic as they offer the best possible data to conduct our research.\\',   \\'Evaluation\\': \\'The evaluation involved comparing how well the spectra fit the models. The spectra were fitted to these models and the quality of fit was assessed based on the deviation of the observed data from the model predictions. We utilized other models for comparison, including the standard disk model and the slim disk model. These models were chosen because of their widespread use in similar research scenarios, making them suitable benchmarks for our model. The comparison process was designed to ensure that our model performs at par, if not better, with existing models.\\',   \\'Results\\': \\'Our model provided a good fit for the observational data, yielding reasonable estimates for the color correction factor. However, it failed to provide an adequate fit for the high-soft-state continuum, a state where the source emits a large amount of soft X-rays. This suggests that the current disk models, including ours, may not fully capture all the complexities of the radiation process. This realization underscores the need for further refinement of our model, or the exploration of alternative models that can better represent the high-soft-state continuum. The implications of these results extend beyond our study, indicating a gap in the existing algorithms that model black holes and their radiation process. As for future studies, it would be beneficial to focus on refining the parameters of the model which deal with high-soft-state continuum.\\'}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from preprocessing import Hypothesis\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def improve_hypothesis(hypothesis: str, n_iters: int = 3) -> str:\n",
    "    for i in tqdm(range(n_iters)):\n",
    "        revised_hypothesis = adversarial_update_hypothesis(hypothesis)\n",
    "        hyp_str = revised_hypothesis['revised_hypothesis']\n",
    "        hyp_str = hyp_str.replace('\\n', ' ')\n",
    "        # Remove everything before the first { and after the last }\n",
    "        hyp_str = re.sub(r'^.*?\\{', '{', hyp_str)\n",
    "        hypothesis = hyp_str\n",
    "    return hypothesis\n",
    "\n",
    "hypothesis_to_improve = {\n",
    "    \"Problem\": \"Issues in determining basic characteristics of black holes and their surrounding disks in X-ray binary, using models of the source's disk X-ray continuum. A key issue is the determination of the \\\"color correction factor\\\".\",\n",
    "    \"Solution\": \"Using observational data to estimate the color correction factor by modeling the disk spectrum with saturated Compton scattering.\",\n",
    "    \"Methodology\": \"The work is based on two observations made by XMM-Newton on GX 339-4. These observations offer high-quality data at low energies. The spectra were then fitted to these models.\",\n",
    "    \"Evaluation\": \"The quality of fit of the spectra to the models was examined. Other models were also tested for fit.\",\n",
    "    \"Results\": \"The spectra fits well with the model and provides reasonable values for the color correction factor. However, the high-soft-state continuum cannot be adequately fitted by the latest disk models.\"\n",
    "}\n",
    "\n",
    "improve_hypothesis(hypothesis=hypothesis_to_improve, n_iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>bit</th>\n",
       "      <th>flip</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Statistical learning traditionally assumes per...</td>\n",
       "      <td>Statistical learning can be achieved even when...</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The conventional belief is that the design of ...</td>\n",
       "      <td>The innovative approach is to design the senso...</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The conventional belief is that in the online ...</td>\n",
       "      <td>The innovative approach is an algorithm that a...</td>\n",
       "      <td>The on-line shortest path problem under partia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Traditional neural networks are not suitable f...</td>\n",
       "      <td>A modified neural network approach, NNRank, ca...</td>\n",
       "      <td>A neural network approach to ordinal regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Monte Carlo Optimization, Parametric machine-L...</td>\n",
       "      <td>Monte Carlo Optimization is mathematically ide...</td>\n",
       "      <td>Parametric Learning and Monte Carlo Optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>2996</td>\n",
       "      <td>The conventional belief is that using a single...</td>\n",
       "      <td>The innovative approach is to average over pre...</td>\n",
       "      <td>Multiple decision trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>2997</td>\n",
       "      <td>Independent Component Analysis (ICA) is an eff...</td>\n",
       "      <td>A kernel ICA model with reconstruction constra...</td>\n",
       "      <td>Kernel Reconstruction ICA for Sparse Represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>2998</td>\n",
       "      <td>The learning of a binary perceptron network is...</td>\n",
       "      <td>The solution space of a binary perceptron netw...</td>\n",
       "      <td>Entropy landscape of solutions in the binary p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>2999</td>\n",
       "      <td>Calibration and evaluation of systems in NIST'...</td>\n",
       "      <td>Both calibration and evaluation require very l...</td>\n",
       "      <td>The BOSARIS Toolkit: Theory, Algorithms and Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>3000</td>\n",
       "      <td>Online learning algorithms are typically desig...</td>\n",
       "      <td>Online mirror descent can be generalized to ti...</td>\n",
       "      <td>A Generalized Online Mirror Descent with Appli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2992 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                bit  \\\n",
       "0         0  Statistical learning traditionally assumes per...   \n",
       "1         1  The conventional belief is that the design of ...   \n",
       "2         2  The conventional belief is that in the online ...   \n",
       "3         3  Traditional neural networks are not suitable f...   \n",
       "4         4  Monte Carlo Optimization, Parametric machine-L...   \n",
       "...     ...                                                ...   \n",
       "2987   2996  The conventional belief is that using a single...   \n",
       "2988   2997  Independent Component Analysis (ICA) is an eff...   \n",
       "2989   2998  The learning of a binary perceptron network is...   \n",
       "2990   2999  Calibration and evaluation of systems in NIST'...   \n",
       "2991   3000  Online learning algorithms are typically desig...   \n",
       "\n",
       "                                                   flip  \\\n",
       "0     Statistical learning can be achieved even when...   \n",
       "1     The innovative approach is to design the senso...   \n",
       "2     The innovative approach is an algorithm that a...   \n",
       "3     A modified neural network approach, NNRank, ca...   \n",
       "4     Monte Carlo Optimization is mathematically ide...   \n",
       "...                                                 ...   \n",
       "2987  The innovative approach is to average over pre...   \n",
       "2988  A kernel ICA model with reconstruction constra...   \n",
       "2989  The solution space of a binary perceptron netw...   \n",
       "2990  Both calibration and evaluation require very l...   \n",
       "2991  Online mirror descent can be generalized to ti...   \n",
       "\n",
       "                                                  title  \n",
       "0                 Learning from compressed observations  \n",
       "1     Sensor Networks with Random Links: Topology De...  \n",
       "2     The on-line shortest path problem under partia...  \n",
       "3       A neural network approach to ordinal regression  \n",
       "4      Parametric Learning and Monte Carlo Optimization  \n",
       "...                                                 ...  \n",
       "2987                            Multiple decision trees  \n",
       "2988  Kernel Reconstruction ICA for Sparse Represent...  \n",
       "2989  Entropy landscape of solutions in the binary p...  \n",
       "2990  The BOSARIS Toolkit: Theory, Algorithms and Co...  \n",
       "2991  A Generalized Online Mirror Descent with Appli...  \n",
       "\n",
       "[2992 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/tuning/cs.LG.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Residual Component Analysis: Generalising PCA for more flexible\n",
      "  inference in linear-Gaussian models\n",
      "---------------------------------------------------------------------------\n",
      "Bit: Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set by solving an eigenvalue problem on the sample covariance matrix, assuming independent spherical Gaussian noise.\n",
      "\n",
      "Flip: Instead of solely relying on PPCA, the data variance can be further decomposed into its components through a generalised eigenvalue problem, called residual component analysis (RCA), which considers other factors like sparse conditional dependencies and temporal correlations that leave some residual variance.\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample a row from the dataframe\n",
    "# Generate random number in (0, len(df))\n",
    "import random\n",
    "\n",
    "random_idx = random.randint(0, len(df)-1)\n",
    "print('-'*75)\n",
    "print(df.iloc[random_idx]['title'])\n",
    "print('-'*75)\n",
    "print(\"Bit: \"+df.iloc[random_idx]['bit']+\"\\n\")\n",
    "print(\"Flip: \"+df.iloc[random_idx]['flip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maxim Raginsky</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "      <td>10.1109/ITW.2007.4313111</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>The problem of statistical learning is to co...</td>\n",
       "      <td>[['Raginsky', 'Maxim', '']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Soummya Kar and Jose M. F. Moura</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "      <td>10.1109/TSP.2008.920143</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>In a sensor network, in practice, the commun...</td>\n",
       "      <td>[['Kar', 'Soummya', ''], ['Moura', 'Jose M. F....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andras Gyorgy, Tamas Linder, Gabor Lugosi, Gyo...</td>\n",
       "      <td>The on-line shortest path problem under partia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG cs.SC</td>\n",
       "      <td>The on-line shortest path problem is conside...</td>\n",
       "      <td>[['Gyorgy', 'Andras', ''], ['Linder', 'Tamas',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jianlin Cheng</td>\n",
       "      <td>A neural network approach to ordinal regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG cs.AI cs.NE</td>\n",
       "      <td>Ordinal regression is an important type of l...</td>\n",
       "      <td>[['Cheng', 'Jianlin', '']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David H. Wolpert and Dev G. Rajnarayan</td>\n",
       "      <td>Parametric Learning and Monte Carlo Optimization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>This paper uncovers and explores the close r...</td>\n",
       "      <td>[['Wolpert', 'David H.', ''], ['Rajnarayan', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0                                     Maxim Raginsky   \n",
       "1                   Soummya Kar and Jose M. F. Moura   \n",
       "2  Andras Gyorgy, Tamas Linder, Gabor Lugosi, Gyo...   \n",
       "3                                      Jianlin Cheng   \n",
       "4             David H. Wolpert and Dev G. Rajnarayan   \n",
       "\n",
       "                                               title  \\\n",
       "0              Learning from compressed observations   \n",
       "1  Sensor Networks with Random Links: Topology De...   \n",
       "2  The on-line shortest path problem under partia...   \n",
       "3    A neural network approach to ordinal regression   \n",
       "4   Parametric Learning and Monte Carlo Optimization   \n",
       "\n",
       "                        doi           categories  \\\n",
       "0  10.1109/ITW.2007.4313111  cs.IT cs.LG math.IT   \n",
       "1   10.1109/TSP.2008.920143  cs.IT cs.LG math.IT   \n",
       "2                       NaN          cs.LG cs.SC   \n",
       "3                       NaN    cs.LG cs.AI cs.NE   \n",
       "4                       NaN                cs.LG   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    The problem of statistical learning is to co...   \n",
       "1    In a sensor network, in practice, the commun...   \n",
       "2    The on-line shortest path problem is conside...   \n",
       "3    Ordinal regression is an important type of l...   \n",
       "4    This paper uncovers and explores the close r...   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0                        [['Raginsky', 'Maxim', '']]  \n",
       "1  [['Kar', 'Soummya', ''], ['Moura', 'Jose M. F....  \n",
       "2  [['Gyorgy', 'Andras', ''], ['Linder', 'Tamas',...  \n",
       "3                         [['Cheng', 'Jianlin', '']]  \n",
       "4  [['Wolpert', 'David H.', ''], ['Rajnarayan', '...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/processed/arxiv-CS.LG.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147502, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import yaml\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader  # Assuming CSVLoader and Document are already imported\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load configuration and set API key\n",
    "config = yaml.safe_load(open(\"../config.yml\"))\n",
    "API_KEY = config['embedding_api_key']\n",
    "DEPLOYMENT_NAME = \"embedding\"\n",
    "BASE_URL = config['embedding_base_url']\n",
    "API_VERSION = config['api_version']\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_base=BASE_URL,\n",
    "    deployment=DEPLOYMENT_NAME,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    "    chunk_size=16,\n",
    ")\n",
    "\n",
    "# Initialize chat model\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=config['base_url'],\n",
    "    openai_api_version=API_VERSION,\n",
    "    deployment_name=config['deployment_name'],\n",
    "    openai_api_key=config['api_key'],\n",
    "    openai_api_type=\"azure\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Load Documents\n",
    "loader = CSVLoader(file_path='../data/processed/arxiv-cs.LG.csv', source_column=\"authors\")\n",
    "documents = loader.load()\n",
    "documents = documents[:50]\n",
    "\n",
    "# Create Chroma Vectorstore\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents, embedding=embeddings,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a Neurips paper review determining the novelty of an idea. Use the following pieces of context to determine if the following idea.\n",
    "\n",
    "Please perform a concise analysis to evaluate the novelty of this \"flip\" by:\n",
    "\n",
    "* Describing the originality of the new idea in relation to the original \"bit\".\n",
    "* Comparing it to existing ideas or implementations that are similar.\n",
    "* Assessing its potential impact or utility in its domain.\n",
    "* Summing up the above findings.\n",
    "\n",
    "Here are some other relevant ideas from paper abstracts that you can use for assessing novelty:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the bit-flip: {question}\n",
    "\n",
    "Based on this context and your own knowledge, determine the novelty of the flip component of the idea. Please provide your final output as a novelty score between 0 to 5, where 0 means \"Not Novel\" and 5 means \"Highly Novel\". \n",
    "If the idea already exists in the provided abstracts, provide a low score. If the idea is incremental or not very different, also provide a low score.\n",
    "Your final output should be of the form 'Novelty Score: x'.\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectordb.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "bit_flip = {\n",
    "    'bit': \"\"\"Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set by solving an eigenvalue problem on the sample covariance matrix, assuming independent spherical Gaussian noise.\"\"\",\n",
    "    'flip': \"\"\"Instead of solely relying on PPCA, the data variance can be further decomposed into its components through a generalised eigenvalue problem, called residual component analysis (RCA), which considers other factors like sparse conditional dependencies and temporal correlations that leave some residual variance.\"\"\",\n",
    "}\n",
    "\n",
    "query = f\"Bit: {bit_flip['bit']}. Flip: {bit_flip['flip']}\"\n",
    "output = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_novelty_score(output):\n",
    "    match = re.search(r'Novelty Score: (\\d)', output)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return \"Could not extract novelty score.\"\n",
    "\n",
    "parse_novelty_score(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Embeddings: 100%|██████████| 50/50 [00:03<00:00, 13.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "\n",
    "def worker(text):\n",
    "    \"\"\"Worker function to compute embeddings for a text.\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    embedded_text = embeddings.embed_query(text)\n",
    "    return (text, embedded_text)\n",
    "\n",
    "# Load configuration and set API key\n",
    "config = yaml.safe_load(open(\"../config.yml\"))\n",
    "API_KEY = config['embedding_api_key']\n",
    "DEPLOYMENT_NAME = \"embedding\"\n",
    "BASE_URL = config['embedding_base_url']\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_base=BASE_URL,\n",
    "    deployment=DEPLOYMENT_NAME,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "# Load CSV file directly\n",
    "df = pd.read_csv('../data/processed/arxiv-cs.LG.csv', low_memory=False)\n",
    "\n",
    "# Create a list of strings by concatenating 'title' and 'abstract'\n",
    "documents = [f\"{row['title']} {row['abstract']}\" for index, row in df.head(50).iterrows()]\n",
    "\n",
    "# Initialize an empty list to hold (text, embedding) tuples\n",
    "text_embeddings = []\n",
    "\n",
    "# Use ThreadPoolExecutor with max_workers for threading\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(tqdm(executor.map(worker, documents), total=len(documents), desc=\"Computing Embeddings\"))\n",
    "    text_embeddings.extend([(text, embedding) for text, embedding in results if text and embedding])\n",
    "\n",
    "# Create a FAISS Vectorstore and add the embeddings\n",
    "vectordb = FAISS.from_embeddings(text_embeddings, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Gaussian processes\"\n",
    "docs = vectordb.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Model Selection Through Sparse Maximum Likelihood Estimation   We consider the problem of estimating the parameters of a Gaussian or binary\\ndistribution in such a way that the resulting undirected graphical model is\\nsparse. Our approach is to solve a maximum likelihood problem with an added\\nl_1-norm penalty term. The problem as formulated is convex but the memory\\nrequirements and complexity of existing interior point methods are prohibitive\\nfor problems with more than tens of nodes. We present two new algorithms for\\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\\nalgorithm uses block coordinate descent, and can be interpreted as recursive\\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\\norder method, yields a complexity estimate with a better dependence on problem\\nsize than existing interior point methods. Using a log determinant relaxation\\nof the log partition function (Wainwright & Jordan (2006)), we show that these\\nsame algorithms can be used to solve an approximate sparse maximum likelihood\\nproblem for the binary case. We test our algorithms on synthetic data, as well\\nas on gene expression and senate voting records data.\\n\", metadata={})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Model Selection Through Sparse Maximum Likelihood Estimation   We consider the problem of estimating the parameters of a Gaussian or binary\\ndistribution in such a way that the resulting undirected graphical model is\\nsparse. Our approach is to solve a maximum likelihood problem with an added\\nl_1-norm penalty term. The problem as formulated is convex but the memory\\nrequirements and complexity of existing interior point methods are prohibitive\\nfor problems with more than tens of nodes. We present two new algorithms for\\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\\nalgorithm uses block coordinate descent, and can be interpreted as recursive\\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\\norder method, yields a complexity estimate with a better dependence on problem\\nsize than existing interior point methods. Using a log determinant relaxation\\nof the log partition function (Wainwright & Jordan (2006)), we show that these\\nsame algorithms can be used to solve an approximate sparse maximum likelihood\\nproblem for the binary case. We test our algorithms on synthetic data, as well\\nas on gene expression and senate voting records data.\\n\", metadata={})"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db = FAISS.load_local(\"../data/vectorstore/arxiv-cs.LG\", embeddings)\n",
    "query = \"Gaussian processes\"\n",
    "docs = new_db.similarity_search(query)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
