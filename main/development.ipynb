{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Critic Chain: Takes a hypothesis and provides a critique.\n",
    "critic_template = \"\"\"\n",
    "Given the following hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "Analyze each part of the hypothesis:\n",
    "1. Clarity: Are the statements clear and easy to understand?\n",
    "2. Coherence: Do the different parts of the hypothesis logically flow together? Does it seem like the proposed solution would work?\n",
    "3. Scientific Validity: Are there any scientific inaccuracies or assumptions that seem unfounded?\n",
    "\n",
    "After your analysis, provide specific feedback on how each field (Problem, Solution, Methodology, Evaluation, Results) can be improved.\n",
    "\"\"\"\n",
    "critic_prompt = PromptTemplate(input_variables=[\"hypothesis\"], template=critic_template)\n",
    "critic_chain = LLMChain(llm=llm, prompt=critic_prompt, output_key=\"critique\")\n",
    "\n",
    "# Reviser Chain: Takes original hypothesis and critique, then provides a revised hypothesis.\n",
    "reviser_template = \"\"\"\n",
    "Given the original hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "And based on the critique provided:\n",
    "{critique}\n",
    "\n",
    "Revise the hypothesis by addressing each point of the critique. Ensure the new version is clearer, more coherent, and scientifically valid.\n",
    "\"\"\"\n",
    "reviser_prompt = PromptTemplate(input_variables=[\"hypothesis\", \"critique\"], template=reviser_template)\n",
    "reviser_chain = LLMChain(llm=llm, prompt=reviser_prompt, output_key=\"revised_hypothesis\")\n",
    "\n",
    "# Overall Sequential Chain: Critiques the hypothesis and then revises it.\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[critic_chain, reviser_chain],\n",
    "    input_variables=[\"hypothesis\"],\n",
    "    output_variables=[\"critique\", \"revised_hypothesis\"],\n",
    "    verbose=True)\n",
    "\n",
    "# Test the chain with a provided hypothesis\n",
    "hypothesis = {\n",
    "    \"Problem\": \"Issues in determining basic characteristics of black holes and their surrounding disks in X-ray binary, using models of the source's disk X-ray continuum. A key issue is the determination of the \\\"color correction factor\\\".\",\n",
    "    \"Solution\": \"Using observational data to estimate the color correction factor by modeling the disk spectrum with saturated Compton scattering.\",\n",
    "    \"Methodology\": \"The work is based on two observations made by XMM-Newton on GX 339-4. These observations offer high-quality data at low energies. The spectra were then fitted to these models.\",\n",
    "    \"Evaluation\": \"The quality of fit of the spectra to the models was examined. Other models were also tested for fit.\",\n",
    "    \"Results\": \"The spectra fits well with the model and provides reasonable values for the color correction factor. However, the high-soft-state continuum cannot be adequately fitted by the latest disk models.\"\n",
    "}\n",
    "result = overall_chain({\"hypothesis\": str(hypothesis)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis:\n",
      "\n",
      "1. Clarity: The hypothesis is relatively clear but contains technical terms that may be difficult for non-experts to understand. For example, terms like \"X-ray binary\", \"color correction factor\", \"saturated Compton scattering\", and \"high-soft-state continuum\" may require more explanation or context.\n",
      "   \n",
      "2. Coherence: The different parts of the hypothesis logically flow together. The problem is clearly stated, a solution is proposed, the methodology is explained, an evaluation process is detailed, and results are provided. \n",
      "\n",
      "3. Scientific Validity: The hypothesis appears to be scientifically valid. It uses real-world observational data and incorporates established scientific models. However, the validity of the results would depend on the accuracy and reliability of the data and models used.\n",
      "\n",
      "Feedback:\n",
      "\n",
      "Problem: The problem statement could be improved by providing more background information to help non-experts understand the issue. For example, briefly explaining what a black hole is, what an X-ray binary is, and why determining their characteristics is important.\n",
      "\n",
      "Solution: The solution could be enhanced by explaining why modeling the disk spectrum with saturated Compton scattering is an effective approach to estimating the color correction factor.\n",
      "\n",
      "Methodology: The methodology could be improved by providing more details about the process. For example, how were the spectra fitted to the models? What specific models were used? \n",
      "\n",
      "Evaluation: The evaluation could be enhanced by specifying the metrics or criteria used to assess the quality of fit. Also, it would be beneficial to know which other models were tested.\n",
      "\n",
      "Results: The results could be improved by providing more specific information. For instance, what are the \"reasonable values\" for the color correction factor? In what ways were the latest disk models unable to adequately fit the high-soft-state continuum?\n"
     ]
    }
   ],
   "source": [
    "print(result['critique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Problem': 'The study aims to determine basic attributes of black holes and their surrounding disks, specifically in X-ray binary systems. In simple terms, X-ray binaries are systems where a black hole and a star are in close proximity, and the intense gravitational pull of the black hole causes matter from the star to form a disk around the black hole. The main challenge is the determination of the \"color correction factor\", a numerical factor that helps us to understand the temperature and energy of the emitting source.', \n",
      "\n",
      "'Solution': 'The proposed solution is to estimate the color correction factor using observational data. This is achieved by modeling the spectrum of the disk around the black hole with a phenomenon known as saturated Compton scattering, where photons gain energy by interacting with high-energy particles.', \n",
      "\n",
      "'Methodology': 'The research utilized two observational datasets provided by the XMM-Newton space telescope on the X-ray binary system GX 339-4. The high-quality, low-energy data from these observations was analyzed and the spectrum of the disk was fitted to models based on the principles of saturated Compton scattering.', \n",
      "\n",
      "'Evaluation': 'The evaluation of this method involved checking the quality of the fit of the observed spectra to the proposed models. The quality of fit was assessed based on the residual sum of squares, a common metric used in spectral analysis. Furthermore, the fit was also compared with other existing models for black hole disks.', \n",
      "\n",
      "'Results': 'The observational spectra fit well with the proposed model, providing color correction factor values within the expected range for X-ray binaries. However, when applied to the high-soft-state continuum, a condition where the X-ray output is dominated by thermal emissions from the disk, the most recent models for black hole disks were found to be inadequate.'}\n"
     ]
    }
   ],
   "source": [
    "print(result['revised_hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "hypothesis_dict = ast.literal_eval(result['revised_hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import yaml\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import SimpleMemory\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This is an LLMChain to write a synopsis given a title of a play.\n",
    "config = yaml.safe_load(open(\"../config.yml\"))\n",
    "API_KEY = config['api_key']\n",
    "DEPLOYMENT_NAME = config['deployment_name']\n",
    "BASE_URL = config['base_url']\n",
    "API_VERSION = config['api_version']\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=BASE_URL,\n",
    "    openai_api_version=API_VERSION,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "# Critic Chain: Takes a hypothesis and provides a critique.\n",
    "critic_template = \"\"\"\n",
    "Given the following hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "Analyze each part of the hypothesis:\n",
    "1. Clarity: Are the statements clear and easy to understand?\n",
    "2. Coherence: Do the different parts of the hypothesis logically flow together? Does it seem like the proposed solution would work?\n",
    "3. Scientific Validity: Are there any scientific inaccuracies or assumptions that seem unfounded?\n",
    "\n",
    "After your analysis, provide specific feedback on how each field (Problem, Solution, Methodology, Evaluation, Results) can be improved.\n",
    "Specifically, give feedback on the frailties of the idea as a whole, and suggest potential enhancements.\n",
    "\"\"\"\n",
    "\n",
    "# Reviser Chain: Takes original hypothesis and critique, then provides a revised hypothesis.\n",
    "reviser_template = \"\"\"\n",
    "Given the original hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "And based on the critique provided:\n",
    "{critique}\n",
    "\n",
    "Revise the hypothesis by addressing each point of the critique. Ensure the new version is clearer, more coherent, and scientifically valid.\n",
    "\"\"\"\n",
    "\n",
    "def adversarial_update_hypothesis(hypothesis):\n",
    "    critic_prompt = PromptTemplate(input_variables=[\"hypothesis\"], template=critic_template)\n",
    "    critic_chain = LLMChain(llm=llm, prompt=critic_prompt, output_key=\"critique\")\n",
    "    reviser_prompt = PromptTemplate(input_variables=[\"hypothesis\", \"critique\"], template=reviser_template)\n",
    "    reviser_chain = LLMChain(llm=llm, prompt=reviser_prompt, output_key=\"revised_hypothesis\")\n",
    "    overall_chain = SequentialChain(\n",
    "        chains=[critic_chain, reviser_chain],\n",
    "        input_variables=[\"hypothesis\"],\n",
    "        output_variables=[\"critique\", \"revised_hypothesis\"],\n",
    "        verbose=False)\n",
    "    result = overall_chain({\"hypothesis\": str(hypothesis)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00c076dca1744538db6e9378afc34c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'{\\'Problem\\': \\'We are tasked with the challenge of accurately determining the fundamental features of black holes and the disks that surround them in X-ray binary, a type of binary star system that emits X-rays. This process is pivotal to enhancing our understanding of how matter behaves in extreme gravitational fields and the nature of black holes. The importance of achieving this lies in its potential to contribute significantly to the broader field of astrophysics, enabling us to understand the universe better. The critical part of this process is the determination of the \"color correction factor\", a measure of how the color of an object appears to change due to relativistic effects when it is moving at speeds close to the speed of light.\\',   \\'Solution\\': \\'We propose a solution that utilizes observational data to derive estimates for the color correction factor. This involves modeling the disk spectrum through the innovative concept of \"saturated Compton scattering\". In this process, photons gain energy by interacting with charged particles, allowing us to account for complexities in the radiation process. Saturated Compton scattering is particularly useful since it provides a more precise estimation of the color correction factor by accounting for the shifts in energy and wavelength of emitted photons. The advantage of this method lies in its ability to provide a more refined and accurate estimate of the color correction factor, which is central to our understanding of black holes.\\',   \\'Methodology\\': \\'Our methodology revolved around using two specific observations made by the XMM-Newton on the X-ray source, GX 339-4. The choice of GX 339-4 was based on its ability to provide high-quality data at low energies, essential for an accurate estimation of the color correction factor. Low-energy observations offer clearer spectral lines, which allows for more accurate modeling and estimation. The selection of these specific observations was strategic as they offer the best possible data to conduct our research.\\',   \\'Evaluation\\': \\'The evaluation involved comparing how well the spectra fit the models. The spectra were fitted to these models and the quality of fit was assessed based on the deviation of the observed data from the model predictions. We utilized other models for comparison, including the standard disk model and the slim disk model. These models were chosen because of their widespread use in similar research scenarios, making them suitable benchmarks for our model. The comparison process was designed to ensure that our model performs at par, if not better, with existing models.\\',   \\'Results\\': \\'Our model provided a good fit for the observational data, yielding reasonable estimates for the color correction factor. However, it failed to provide an adequate fit for the high-soft-state continuum, a state where the source emits a large amount of soft X-rays. This suggests that the current disk models, including ours, may not fully capture all the complexities of the radiation process. This realization underscores the need for further refinement of our model, or the exploration of alternative models that can better represent the high-soft-state continuum. The implications of these results extend beyond our study, indicating a gap in the existing algorithms that model black holes and their radiation process. As for future studies, it would be beneficial to focus on refining the parameters of the model which deal with high-soft-state continuum.\\'}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from preprocessing import Hypothesis\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def improve_hypothesis(hypothesis: str, n_iters: int = 3) -> str:\n",
    "    for i in tqdm(range(n_iters)):\n",
    "        revised_hypothesis = adversarial_update_hypothesis(hypothesis)\n",
    "        hyp_str = revised_hypothesis['revised_hypothesis']\n",
    "        hyp_str = hyp_str.replace('\\n', ' ')\n",
    "        # Remove everything before the first { and after the last }\n",
    "        hyp_str = re.sub(r'^.*?\\{', '{', hyp_str)\n",
    "        hypothesis = hyp_str\n",
    "    return hypothesis\n",
    "\n",
    "hypothesis_to_improve = {\n",
    "    \"Problem\": \"Issues in determining basic characteristics of black holes and their surrounding disks in X-ray binary, using models of the source's disk X-ray continuum. A key issue is the determination of the \\\"color correction factor\\\".\",\n",
    "    \"Solution\": \"Using observational data to estimate the color correction factor by modeling the disk spectrum with saturated Compton scattering.\",\n",
    "    \"Methodology\": \"The work is based on two observations made by XMM-Newton on GX 339-4. These observations offer high-quality data at low energies. The spectra were then fitted to these models.\",\n",
    "    \"Evaluation\": \"The quality of fit of the spectra to the models was examined. Other models were also tested for fit.\",\n",
    "    \"Results\": \"The spectra fits well with the model and provides reasonable values for the color correction factor. However, the high-soft-state continuum cannot be adequately fitted by the latest disk models.\"\n",
    "}\n",
    "\n",
    "improve_hypothesis(hypothesis=hypothesis_to_improve, n_iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12057 hypotheses in the dataset.\n",
      "There are 147502 abstracts in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/tuning/cs.LG.csv\")\n",
    "print(f\"There are {len(df)} hypotheses in the dataset.\")\n",
    "abstracts = pd.read_csv(\"../data/processed/arxiv-cs.LG.csv\", low_memory=False)\n",
    "print(f\"There are {len(abstracts)} abstracts in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Online Learning in Decentralized Multiuser Resource Sharing Problems\n",
      "---------------------------------------------------------------------------\n",
      "Bit: In decentralized systems, resource sharing is typically handled without considering the time-varying and unknown qualities of the resources. The traditional approach assumes that users operate independently, with no communication between them, and that the use of a resource by multiple users leads to reduced quality due to congestion. This approach also overlooks the potential benefits of user-specific rewards and the cooperative goal of achieving the highest system utility.\n",
      "\n",
      "Flip: This research proposes a shift towards considering the time-varying and unknown qualities of resources in a decentralized system. It suggests the implementation of distributed learning algorithms that allow for user-specific rewards and costly communication between users, with the aim of achieving the highest system utility. The proposed approach takes into account the decentralized nature of the system, unknown resource qualities, and communication, computation, and switching costs, and it promises logarithmic regret with respect to the optimal allocation.\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample a row from the dataframe\n",
    "# Generate random number in (0, len(df))\n",
    "import random\n",
    "\n",
    "random_idx = random.randint(0, len(df)-1)\n",
    "print('-'*75)\n",
    "print(df.iloc[random_idx]['title'])\n",
    "print('-'*75)\n",
    "print(\"Bit: \"+df.iloc[random_idx]['bit']+\"\\n\")\n",
    "print(\"Flip: \"+df.iloc[random_idx]['flip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Online Learning in Decentralized Multiuser Resource Sharing Problems\n",
      "\n",
      "  In this paper, we consider the general scenario of resource sharing in a\n",
      "decentralized system when the resource rewards/qualities are time-varying and\n",
      "unknown to the users, and using the same resource by multiple users leads to\n",
      "reduced quality due to resource sharing. Firstly, we consider a\n",
      "user-independent reward model with no communication between the users, where a\n",
      "user gets feedback about the congestion level in the resource it uses.\n",
      "Secondly, we consider user-specific rewards and allow costly communication\n",
      "between the users. The users have a cooperative goal of achieving the highest\n",
      "system utility. There are multiple obstacles in achieving this goal such as the\n",
      "decentralized nature of the system, unknown resource qualities, communication,\n",
      "computation and switching costs. We propose distributed learning algorithms\n",
      "with logarithmic regret with respect to the optimal allocation. Our logarithmic\n",
      "regret result holds under both i.i.d. and Markovian reward models, as well as\n",
      "under communication, computation and switching costs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the original abstract using the index\n",
    "title = df.iloc[random_idx]['title']\n",
    "corresponding_idx = int(abstracts[abstracts.title == title].index[0])\n",
    "print('-'*75)\n",
    "print(abstracts.iloc[corresponding_idx]['title'])\n",
    "print()\n",
    "print(abstracts.iloc[corresponding_idx]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maxim Raginsky</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "      <td>10.1109/ITW.2007.4313111</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>The problem of statistical learning is to co...</td>\n",
       "      <td>[['Raginsky', 'Maxim', '']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Soummya Kar and Jose M. F. Moura</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "      <td>10.1109/TSP.2008.920143</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>In a sensor network, in practice, the commun...</td>\n",
       "      <td>[['Kar', 'Soummya', ''], ['Moura', 'Jose M. F....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andras Gyorgy, Tamas Linder, Gabor Lugosi, Gyo...</td>\n",
       "      <td>The on-line shortest path problem under partia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG cs.SC</td>\n",
       "      <td>The on-line shortest path problem is conside...</td>\n",
       "      <td>[['Gyorgy', 'Andras', ''], ['Linder', 'Tamas',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jianlin Cheng</td>\n",
       "      <td>A neural network approach to ordinal regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG cs.AI cs.NE</td>\n",
       "      <td>Ordinal regression is an important type of l...</td>\n",
       "      <td>[['Cheng', 'Jianlin', '']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David H. Wolpert and Dev G. Rajnarayan</td>\n",
       "      <td>Parametric Learning and Monte Carlo Optimization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>This paper uncovers and explores the close r...</td>\n",
       "      <td>[['Wolpert', 'David H.', ''], ['Rajnarayan', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0                                     Maxim Raginsky   \n",
       "1                   Soummya Kar and Jose M. F. Moura   \n",
       "2  Andras Gyorgy, Tamas Linder, Gabor Lugosi, Gyo...   \n",
       "3                                      Jianlin Cheng   \n",
       "4             David H. Wolpert and Dev G. Rajnarayan   \n",
       "\n",
       "                                               title  \\\n",
       "0              Learning from compressed observations   \n",
       "1  Sensor Networks with Random Links: Topology De...   \n",
       "2  The on-line shortest path problem under partia...   \n",
       "3    A neural network approach to ordinal regression   \n",
       "4   Parametric Learning and Monte Carlo Optimization   \n",
       "\n",
       "                        doi           categories  \\\n",
       "0  10.1109/ITW.2007.4313111  cs.IT cs.LG math.IT   \n",
       "1   10.1109/TSP.2008.920143  cs.IT cs.LG math.IT   \n",
       "2                       NaN          cs.LG cs.SC   \n",
       "3                       NaN    cs.LG cs.AI cs.NE   \n",
       "4                       NaN                cs.LG   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    The problem of statistical learning is to co...   \n",
       "1    In a sensor network, in practice, the commun...   \n",
       "2    The on-line shortest path problem is conside...   \n",
       "3    Ordinal regression is an important type of l...   \n",
       "4    This paper uncovers and explores the close r...   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0                        [['Raginsky', 'Maxim', '']]  \n",
       "1  [['Kar', 'Soummya', ''], ['Moura', 'Jose M. F....  \n",
       "2  [['Gyorgy', 'Andras', ''], ['Linder', 'Tamas',...  \n",
       "3                         [['Cheng', 'Jianlin', '']]  \n",
       "4  [['Wolpert', 'David H.', ''], ['Rajnarayan', '...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/processed/arxiv-CS.LG.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147502, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import yaml\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader  # Assuming CSVLoader and Document are already imported\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load configuration and set API key\n",
    "config = yaml.safe_load(open(\"../config.yml\"))\n",
    "API_KEY = config['embedding_api_key']\n",
    "DEPLOYMENT_NAME = \"embedding\"\n",
    "BASE_URL = config['embedding_base_url']\n",
    "API_VERSION = config['api_version']\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_base=BASE_URL,\n",
    "    deployment=DEPLOYMENT_NAME,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    "    chunk_size=16,\n",
    ")\n",
    "\n",
    "# Initialize chat model\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=config['base_url'],\n",
    "    openai_api_version=API_VERSION,\n",
    "    deployment_name=config['deployment_name'],\n",
    "    openai_api_key=config['api_key'],\n",
    "    openai_api_type=\"azure\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Load Documents\n",
    "loader = CSVLoader(file_path='../data/processed/arxiv-cs.LG.csv', source_column=\"authors\")\n",
    "documents = loader.load()\n",
    "documents = documents[:50]\n",
    "\n",
    "# Create Chroma Vectorstore\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents, embedding=embeddings,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a Neurips paper review determining the novelty of an idea. Use the following pieces of context to determine if the following idea.\n",
    "\n",
    "Please perform a concise analysis to evaluate the novelty of this \"flip\" by:\n",
    "\n",
    "* Describing the originality of the new idea in relation to the original \"bit\".\n",
    "* Comparing it to existing ideas or implementations that are similar.\n",
    "* Assessing its potential impact or utility in its domain.\n",
    "* Summing up the above findings.\n",
    "\n",
    "Here are some other relevant ideas from paper abstracts that you can use for assessing novelty:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the bit-flip: {question}\n",
    "\n",
    "Based on this context and your own knowledge, determine the novelty of the flip component of the idea. Please provide your final output as a novelty score between 0 to 5, where 0 means \"Not Novel\" and 5 means \"Highly Novel\". \n",
    "If the idea already exists in the provided abstracts, provide a low score. If the idea is incremental or not very different, also provide a low score.\n",
    "Your final output should be of the form 'Novelty Score: x'.\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectordb.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "bit_flip = {\n",
    "    'bit': \"\"\"Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set by solving an eigenvalue problem on the sample covariance matrix, assuming independent spherical Gaussian noise.\"\"\",\n",
    "    'flip': \"\"\"Instead of solely relying on PPCA, the data variance can be further decomposed into its components through a generalised eigenvalue problem, called residual component analysis (RCA), which considers other factors like sparse conditional dependencies and temporal correlations that leave some residual variance.\"\"\",\n",
    "}\n",
    "\n",
    "query = f\"Bit: {bit_flip['bit']}. Flip: {bit_flip['flip']}\"\n",
    "output = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_novelty_score(output):\n",
    "    match = re.search(r'Novelty Score: (\\d)', output)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return \"Could not extract novelty score.\"\n",
    "\n",
    "parse_novelty_score(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Embeddings: 100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "\n",
    "def worker(text):\n",
    "    \"\"\"Worker function to compute embeddings for a text.\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    embedded_text = embeddings.embed_query(text)\n",
    "    return (text, embedded_text)\n",
    "\n",
    "# Load configuration and set API key\n",
    "config = yaml.safe_load(open(\"../config.yml\"))\n",
    "API_KEY = config['embedding_api_key']\n",
    "DEPLOYMENT_NAME = \"embedding\"\n",
    "BASE_URL = config['embedding_base_url']\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_base=BASE_URL,\n",
    "    deployment=DEPLOYMENT_NAME,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "# Load CSV file directly\n",
    "df = pd.read_csv('../data/processed/arxiv-cs.LG.csv', low_memory=False)\n",
    "\n",
    "# Create a list of strings by concatenating 'title' and 'abstract'\n",
    "documents = [f\"{row['title']} {row['abstract']}\" for index, row in df.head(50).iterrows()]\n",
    "\n",
    "# Initialize an empty list to hold (text, embedding) tuples\n",
    "text_embeddings = []\n",
    "\n",
    "# Use ThreadPoolExecutor with max_workers for threading\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(tqdm(executor.map(worker, documents), total=len(documents), desc=\"Computing Embeddings\"))\n",
    "    text_embeddings.extend([(text, embedding) for text, embedding in results if text and embedding])\n",
    "\n",
    "# Create a FAISS Vectorstore and add the embeddings\n",
    "vectordb = FAISS.from_embeddings(text_embeddings, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Gaussian processes\"\n",
    "docs = vectordb.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Model Selection Through Sparse Maximum Likelihood Estimation   We consider the problem of estimating the parameters of a Gaussian or binary\\ndistribution in such a way that the resulting undirected graphical model is\\nsparse. Our approach is to solve a maximum likelihood problem with an added\\nl_1-norm penalty term. The problem as formulated is convex but the memory\\nrequirements and complexity of existing interior point methods are prohibitive\\nfor problems with more than tens of nodes. We present two new algorithms for\\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\\nalgorithm uses block coordinate descent, and can be interpreted as recursive\\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\\norder method, yields a complexity estimate with a better dependence on problem\\nsize than existing interior point methods. Using a log determinant relaxation\\nof the log partition function (Wainwright & Jordan (2006)), we show that these\\nsame algorithms can be used to solve an approximate sparse maximum likelihood\\nproblem for the binary case. We test our algorithms on synthetic data, as well\\nas on gene expression and senate voting records data.\\n\", metadata={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Gaussian Processes for Nonlinear Signal Processing   Gaussian processes (GPs) are versatile tools that have been successfully\\nemployed to solve nonlinear estimation problems in machine learning, but that\\nare rarely used in signal processing. In this tutorial, we present GPs for\\nregression as a natural nonlinear extension to optimal Wiener filtering. After\\nestablishing their basic formulation, we discuss several important aspects and\\nextensions, including recursive and adaptive algorithms for dealing with\\nnon-stationarity, low-complexity solutions, non-Gaussian noise models and\\nclassification scenarios. Furthermore, we provide a selection of relevant\\napplications to wireless digital communications.\\n', metadata={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db = FAISS.load_local(\"../data/vectorstore/arxiv-cs.LG\", embeddings)\n",
    "query = \"Gaussian processes\"\n",
    "docs = new_db.similarity_search(query)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
