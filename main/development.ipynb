{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/cdt_cw_5265_z_gwnxlf0tv80000gn/T/ipykernel_48419/3535194879.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('arxiv.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(327288, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('arxiv.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"  We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy\\nobservations of the Serpens star-forming region. In particular we present a set\\nof criteria for isolating bona fide young stellar objects, YSO's, from the\\nextensive background contamination by extra-galactic objects. We then discuss\\nthe properties of the resulting high confidence set of YSO's. We find 235 such\\nobjects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An\\nadditional set of 51 lower confidence YSO's outside this area is identified\\nfrom the MIPS data combined with 2MASS photometry. We describe two sets of\\nresults, color-color diagrams to compare our observed source properties with\\nthose of theoretical models for star/disk/envelope systems and our own modeling\\nof the subset of our objects that appear to be star+disks. These objects\\nexhibit a very wide range of disk properties, from many that can be fit with\\nactively accreting disks to some with both passive disks and even possibly\\ndebris disks. We find that the luminosity function of YSO's in Serpens extends\\ndown to at least a few x .001 Lsun or lower for an assumed distance of 260 pc.\\nThe lower limit may be set by our inability to distinguish YSO's from\\nextra-galactic sources more than by the lack of YSO's at very low luminosities.\\nA spatial clustering analysis shows that the nominally less-evolved YSO's are\\nmore highly clustered than the later stages and that the background\\nextra-galactic population can be fit by the same two-point correlation function\\nas seen in other extra-galactic studies. We also present a table of matches\\nbetween several previous infrared and X-ray studies of the Serpens YSO\\npopulation and our Spitzer data set.\\n\",\n",
       "       '  Results from spectroscopic observations of the Intermediate Polar (IP) EX Hya\\nin quiescence during 1991 and 2001 are presented. Spin-modulated radial\\nvelocities consistent with an outer disc origin were detected for the first\\ntime in an IP. The spin pulsation was modulated with velocities near ~500-600\\nkm/s. These velocities are consistent with those of material circulating at the\\nouter edge of the accretion disc, suggesting corotation of the accretion\\ncurtain with material near the Roche lobe radius. Furthermore, spin Doppler\\ntomograms have revealed evidence of the accretion curtain emission extending\\nfrom velocities of ~500 km/s to ~1000 km/s. These findings have confirmed the\\ntheoretical model predictions of King & Wynn (1999), Belle et al. (2002) and\\nNorton et al. (2004) for EX Hya, which predict large accretion curtains that\\nextend to a distance close to the Roche lobe radius in this system. Evidence\\nfor overflow stream of material falling onto the magnetosphere was observed,\\nconfirming the result of Belle et al. (2005) that disc overflow in EX Hya is\\npresent during quiescence as well as outburst. It appears that the hbeta and\\nhgamma spin radial velocities originated from the rotation of the funnel at the\\nouter disc edge, while those of halpha were produced due to the flow of\\nmaterial along the field lines far from the white dwarf (narrow component) and\\nclose to the white dwarf (broad-base component), in agreement with the\\naccretion curtain model.\\n',\n",
       "       '  The very nature of the solar chromosphere, its structuring and dynamics,\\nremains far from being properly understood, in spite of intensive research.\\nHere we point out the potential of chromospheric observations at millimeter\\nwavelengths to resolve this long-standing problem. Computations carried out\\nwith a sophisticated dynamic model of the solar chromosphere due to Carlsson\\nand Stein demonstrate that millimeter emission is extremely sensitive to\\ndynamic processes in the chromosphere and the appropriate wavelengths to look\\nfor dynamic signatures are in the range 0.8-5.0 mm. The model also suggests\\nthat high resolution observations at mm wavelengths, as will be provided by\\nALMA, will have the unique property of reacting to both the hot and the cool\\ngas, and thus will have the potential of distinguishing between rival models of\\nthe solar atmosphere. Thus, initial results obtained from the observations of\\nthe quiet Sun at 3.5 mm with the BIMA array (resolution of 12 arcsec) reveal\\nsignificant oscillations with amplitudes of 50-150 K and frequencies of 1.5-8\\nmHz with a tendency toward short-period oscillations in internetwork and longer\\nperiods in network regions. However higher spatial resolution, such as that\\nprovided by ALMA, is required for a clean separation between the features\\nwithin the solar atmosphere and for an adequate comparison with the output of\\nthe comprehensive dynamic simulations.\\n',\n",
       "       ...,\n",
       "       '  Fidelity plays a key role in quantum information and communication theory.\\nFidelity can be interpreted as the probability that a decoded message possesses\\nthe same information content as the message prior to coding and transmission.\\nIn this paper, we give a formula of Bures fidelity for displaced squeezed\\nthermal states directly by the displacement and squeezing parameters and\\nbirefly discuss how the results can apply to quantum information theory.\\n',\n",
       "       '  Newtonian dynamical systems accepting the normal shift on an arbitrary\\nRiemannian manifold are considered. Partial differential equations forming the\\nweak and additional normality conditions for them are reported.\\n',\n",
       "       '  The vector Burgers equation is extended to include pressure gradients and\\ngravity. It is shown that within the framework of the Cole-Hopf transformation\\nthere are no physical solutions to this problem. This result is important\\nbecause it clearly demonstrates that any extension of Burgers equation to more\\ninteresting physical situations is strongly limited.\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hypotheses extracted: 89\n"
     ]
    }
   ],
   "source": [
    "# Read in extracted_large.json and print how many hypotheses were extracted:\n",
    "import json\n",
    "\n",
    "with open('extracted_large.json', 'r') as f:\n",
    "    hypotheses = json.load(f)\n",
    "\n",
    "print(f\"Number of hypotheses extracted: {len(hypotheses)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Critic Chain: Takes a hypothesis and provides a critique.\n",
    "critic_template = \"\"\"\n",
    "Given the following hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "Analyze each part of the hypothesis:\n",
    "1. Clarity: Are the statements clear and easy to understand?\n",
    "2. Coherence: Do the different parts of the hypothesis logically flow together? Does it seem like the proposed solution would work?\n",
    "3. Scientific Validity: Are there any scientific inaccuracies or assumptions that seem unfounded?\n",
    "\n",
    "After your analysis, provide specific feedback on how each field (Problem, Solution, Methodology, Evaluation, Results) can be improved.\n",
    "\"\"\"\n",
    "critic_prompt = PromptTemplate(input_variables=[\"hypothesis\"], template=critic_template)\n",
    "critic_chain = LLMChain(llm=llm, prompt=critic_prompt, output_key=\"critique\")\n",
    "\n",
    "# Reviser Chain: Takes original hypothesis and critique, then provides a revised hypothesis.\n",
    "reviser_template = \"\"\"\n",
    "Given the original hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "And based on the critique provided:\n",
    "{critique}\n",
    "\n",
    "Revise the hypothesis by addressing each point of the critique. Ensure the new version is clearer, more coherent, and scientifically valid.\n",
    "\"\"\"\n",
    "reviser_prompt = PromptTemplate(input_variables=[\"hypothesis\", \"critique\"], template=reviser_template)\n",
    "reviser_chain = LLMChain(llm=llm, prompt=reviser_prompt, output_key=\"revised_hypothesis\")\n",
    "\n",
    "# Overall Sequential Chain: Critiques the hypothesis and then revises it.\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[critic_chain, reviser_chain],\n",
    "    input_variables=[\"hypothesis\"],\n",
    "    output_variables=[\"critique\", \"revised_hypothesis\"],\n",
    "    verbose=True)\n",
    "\n",
    "# Test the chain with a provided hypothesis\n",
    "hypothesis = {\n",
    "    \"Problem\": \"Issues in determining basic characteristics of black holes and their surrounding disks in X-ray binary, using models of the source's disk X-ray continuum. A key issue is the determination of the \\\"color correction factor\\\".\",\n",
    "    \"Solution\": \"Using observational data to estimate the color correction factor by modeling the disk spectrum with saturated Compton scattering.\",\n",
    "    \"Methodology\": \"The work is based on two observations made by XMM-Newton on GX 339-4. These observations offer high-quality data at low energies. The spectra were then fitted to these models.\",\n",
    "    \"Evaluation\": \"The quality of fit of the spectra to the models was examined. Other models were also tested for fit.\",\n",
    "    \"Results\": \"The spectra fits well with the model and provides reasonable values for the color correction factor. However, the high-soft-state continuum cannot be adequately fitted by the latest disk models.\"\n",
    "}\n",
    "result = overall_chain({\"hypothesis\": str(hypothesis)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis:\n",
      "\n",
      "1. Clarity: The hypothesis is relatively clear but contains technical terms that may be difficult for non-experts to understand. For example, terms like \"X-ray binary\", \"color correction factor\", \"saturated Compton scattering\", and \"high-soft-state continuum\" may require more explanation or context.\n",
      "   \n",
      "2. Coherence: The different parts of the hypothesis logically flow together. The problem is clearly stated, a solution is proposed, the methodology is explained, an evaluation process is detailed, and results are provided. \n",
      "\n",
      "3. Scientific Validity: The hypothesis appears to be scientifically valid. It uses real-world observational data and incorporates established scientific models. However, the validity of the results would depend on the accuracy and reliability of the data and models used.\n",
      "\n",
      "Feedback:\n",
      "\n",
      "Problem: The problem statement could be improved by providing more background information to help non-experts understand the issue. For example, briefly explaining what a black hole is, what an X-ray binary is, and why determining their characteristics is important.\n",
      "\n",
      "Solution: The solution could be enhanced by explaining why modeling the disk spectrum with saturated Compton scattering is an effective approach to estimating the color correction factor.\n",
      "\n",
      "Methodology: The methodology could be improved by providing more details about the process. For example, how were the spectra fitted to the models? What specific models were used? \n",
      "\n",
      "Evaluation: The evaluation could be enhanced by specifying the metrics or criteria used to assess the quality of fit. Also, it would be beneficial to know which other models were tested.\n",
      "\n",
      "Results: The results could be improved by providing more specific information. For instance, what are the \"reasonable values\" for the color correction factor? In what ways were the latest disk models unable to adequately fit the high-soft-state continuum?\n"
     ]
    }
   ],
   "source": [
    "print(result['critique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Problem': 'The study aims to determine basic attributes of black holes and their surrounding disks, specifically in X-ray binary systems. In simple terms, X-ray binaries are systems where a black hole and a star are in close proximity, and the intense gravitational pull of the black hole causes matter from the star to form a disk around the black hole. The main challenge is the determination of the \"color correction factor\", a numerical factor that helps us to understand the temperature and energy of the emitting source.', \n",
      "\n",
      "'Solution': 'The proposed solution is to estimate the color correction factor using observational data. This is achieved by modeling the spectrum of the disk around the black hole with a phenomenon known as saturated Compton scattering, where photons gain energy by interacting with high-energy particles.', \n",
      "\n",
      "'Methodology': 'The research utilized two observational datasets provided by the XMM-Newton space telescope on the X-ray binary system GX 339-4. The high-quality, low-energy data from these observations was analyzed and the spectrum of the disk was fitted to models based on the principles of saturated Compton scattering.', \n",
      "\n",
      "'Evaluation': 'The evaluation of this method involved checking the quality of the fit of the observed spectra to the proposed models. The quality of fit was assessed based on the residual sum of squares, a common metric used in spectral analysis. Furthermore, the fit was also compared with other existing models for black hole disks.', \n",
      "\n",
      "'Results': 'The observational spectra fit well with the proposed model, providing color correction factor values within the expected range for X-ray binaries. However, when applied to the high-soft-state continuum, a condition where the X-ray output is dominated by thermal emissions from the disk, the most recent models for black hole disks were found to be inadequate.'}\n"
     ]
    }
   ],
   "source": [
    "print(result['revised_hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "hypothesis_dict = ast.literal_eval(result['revised_hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import yaml\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import SimpleMemory\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This is an LLMChain to write a synopsis given a title of a play.\n",
    "config = yaml.safe_load(open(\"../config.yml\"))\n",
    "API_KEY = config['api_key']\n",
    "DEPLOYMENT_NAME = config['deployment_name']\n",
    "BASE_URL = config['base_url']\n",
    "API_VERSION = config['api_version']\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=BASE_URL,\n",
    "    openai_api_version=API_VERSION,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "# Critic Chain: Takes a hypothesis and provides a critique.\n",
    "critic_template = \"\"\"\n",
    "Given the following hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "Analyze each part of the hypothesis:\n",
    "1. Clarity: Are the statements clear and easy to understand?\n",
    "2. Coherence: Do the different parts of the hypothesis logically flow together? Does it seem like the proposed solution would work?\n",
    "3. Scientific Validity: Are there any scientific inaccuracies or assumptions that seem unfounded?\n",
    "\n",
    "After your analysis, provide specific feedback on how each field (Problem, Solution, Methodology, Evaluation, Results) can be improved.\n",
    "Specifically, give feedback on the frailties of the idea as a whole, and suggest potential enhancements.\n",
    "\"\"\"\n",
    "\n",
    "# Reviser Chain: Takes original hypothesis and critique, then provides a revised hypothesis.\n",
    "reviser_template = \"\"\"\n",
    "Given the original hypothesis:\n",
    "{hypothesis}\n",
    "\n",
    "And based on the critique provided:\n",
    "{critique}\n",
    "\n",
    "Revise the hypothesis by addressing each point of the critique. Ensure the new version is clearer, more coherent, and scientifically valid.\n",
    "\"\"\"\n",
    "\n",
    "def adversarial_update_hypothesis(hypothesis):\n",
    "    critic_prompt = PromptTemplate(input_variables=[\"hypothesis\"], template=critic_template)\n",
    "    critic_chain = LLMChain(llm=llm, prompt=critic_prompt, output_key=\"critique\")\n",
    "    reviser_prompt = PromptTemplate(input_variables=[\"hypothesis\", \"critique\"], template=reviser_template)\n",
    "    reviser_chain = LLMChain(llm=llm, prompt=reviser_prompt, output_key=\"revised_hypothesis\")\n",
    "    overall_chain = SequentialChain(\n",
    "        chains=[critic_chain, reviser_chain],\n",
    "        input_variables=[\"hypothesis\"],\n",
    "        output_variables=[\"critique\", \"revised_hypothesis\"],\n",
    "        verbose=False)\n",
    "    result = overall_chain({\"hypothesis\": str(hypothesis)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00c076dca1744538db6e9378afc34c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'{\\'Problem\\': \\'We are tasked with the challenge of accurately determining the fundamental features of black holes and the disks that surround them in X-ray binary, a type of binary star system that emits X-rays. This process is pivotal to enhancing our understanding of how matter behaves in extreme gravitational fields and the nature of black holes. The importance of achieving this lies in its potential to contribute significantly to the broader field of astrophysics, enabling us to understand the universe better. The critical part of this process is the determination of the \"color correction factor\", a measure of how the color of an object appears to change due to relativistic effects when it is moving at speeds close to the speed of light.\\',   \\'Solution\\': \\'We propose a solution that utilizes observational data to derive estimates for the color correction factor. This involves modeling the disk spectrum through the innovative concept of \"saturated Compton scattering\". In this process, photons gain energy by interacting with charged particles, allowing us to account for complexities in the radiation process. Saturated Compton scattering is particularly useful since it provides a more precise estimation of the color correction factor by accounting for the shifts in energy and wavelength of emitted photons. The advantage of this method lies in its ability to provide a more refined and accurate estimate of the color correction factor, which is central to our understanding of black holes.\\',   \\'Methodology\\': \\'Our methodology revolved around using two specific observations made by the XMM-Newton on the X-ray source, GX 339-4. The choice of GX 339-4 was based on its ability to provide high-quality data at low energies, essential for an accurate estimation of the color correction factor. Low-energy observations offer clearer spectral lines, which allows for more accurate modeling and estimation. The selection of these specific observations was strategic as they offer the best possible data to conduct our research.\\',   \\'Evaluation\\': \\'The evaluation involved comparing how well the spectra fit the models. The spectra were fitted to these models and the quality of fit was assessed based on the deviation of the observed data from the model predictions. We utilized other models for comparison, including the standard disk model and the slim disk model. These models were chosen because of their widespread use in similar research scenarios, making them suitable benchmarks for our model. The comparison process was designed to ensure that our model performs at par, if not better, with existing models.\\',   \\'Results\\': \\'Our model provided a good fit for the observational data, yielding reasonable estimates for the color correction factor. However, it failed to provide an adequate fit for the high-soft-state continuum, a state where the source emits a large amount of soft X-rays. This suggests that the current disk models, including ours, may not fully capture all the complexities of the radiation process. This realization underscores the need for further refinement of our model, or the exploration of alternative models that can better represent the high-soft-state continuum. The implications of these results extend beyond our study, indicating a gap in the existing algorithms that model black holes and their radiation process. As for future studies, it would be beneficial to focus on refining the parameters of the model which deal with high-soft-state continuum.\\'}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from preprocessing import Hypothesis\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def improve_hypothesis(hypothesis: str, n_iters: int = 3) -> str:\n",
    "    for i in tqdm(range(n_iters)):\n",
    "        revised_hypothesis = adversarial_update_hypothesis(hypothesis)\n",
    "        hyp_str = revised_hypothesis['revised_hypothesis']\n",
    "        hyp_str = hyp_str.replace('\\n', ' ')\n",
    "        # Remove everything before the first { and after the last }\n",
    "        hyp_str = re.sub(r'^.*?\\{', '{', hyp_str)\n",
    "        hypothesis = hyp_str\n",
    "    return hypothesis\n",
    "\n",
    "hypothesis_to_improve = {\n",
    "    \"Problem\": \"Issues in determining basic characteristics of black holes and their surrounding disks in X-ray binary, using models of the source's disk X-ray continuum. A key issue is the determination of the \\\"color correction factor\\\".\",\n",
    "    \"Solution\": \"Using observational data to estimate the color correction factor by modeling the disk spectrum with saturated Compton scattering.\",\n",
    "    \"Methodology\": \"The work is based on two observations made by XMM-Newton on GX 339-4. These observations offer high-quality data at low energies. The spectra were then fitted to these models.\",\n",
    "    \"Evaluation\": \"The quality of fit of the spectra to the models was examined. Other models were also tested for fit.\",\n",
    "    \"Results\": \"The spectra fits well with the model and provides reasonable values for the color correction factor. However, the high-soft-state continuum cannot be adequately fitted by the latest disk models.\"\n",
    "}\n",
    "\n",
    "improve_hypothesis(hypothesis=hypothesis_to_improve, n_iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
